{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de7699b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paras\\AppData\\Local\\Temp\\ipykernel_4384\\2326751504.py:14: DtypeWarning: Columns (7,9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(r\"C:\\Users\\paras\\Documents\\UNSW_2018_IoT_Botnet_Full5pc_4.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully.\n",
      "   pkSeqID         stime flgs  flgs_number proto  proto_number  \\\n",
      "0  3000001  1.528099e+09    e            1   udp             3   \n",
      "1  3000002  1.528099e+09    e            1   udp             3   \n",
      "2  3000003  1.528099e+09    e            1   udp             3   \n",
      "3  3000004  1.528099e+09    e            1   udp             3   \n",
      "4  3000005  1.528099e+09    e            1   udp             3   \n",
      "\n",
      "             saddr sport          daddr dport  ...  AR_P_Proto_P_DstIP  \\\n",
      "0  192.168.100.147  6226  192.168.100.3    80  ...             1.09825   \n",
      "1  192.168.100.147  6227  192.168.100.3    80  ...             1.09825   \n",
      "2  192.168.100.147  6228  192.168.100.3    80  ...             1.09825   \n",
      "3  192.168.100.147  6229  192.168.100.3    80  ...             1.09825   \n",
      "4  192.168.100.147  6230  192.168.100.3    80  ...             1.09825   \n",
      "\n",
      "   N_IN_Conn_P_DstIP N_IN_Conn_P_SrcIP  AR_P_Proto_P_Sport  \\\n",
      "0                100               100             1.09827   \n",
      "1                100               100             1.09827   \n",
      "2                100               100             1.09827   \n",
      "3                100               100             1.09827   \n",
      "4                100               100             1.09827   \n",
      "\n",
      "   AR_P_Proto_P_Dport  Pkts_P_State_P_Protocol_P_DestIP  \\\n",
      "0             1.09825                              1500   \n",
      "1             1.09825                              1500   \n",
      "2             1.09825                              1500   \n",
      "3             1.09825                              1500   \n",
      "4             1.09825                              1500   \n",
      "\n",
      "   Pkts_P_State_P_Protocol_P_SrcIP  attack  category  subcategory  \n",
      "0                             1500       1      DDoS          UDP  \n",
      "1                             1500       1      DDoS          UDP  \n",
      "2                             1500       1      DDoS          UDP  \n",
      "3                             1500       1      DDoS          UDP  \n",
      "4                             1500       1      DDoS          UDP  \n",
      "\n",
      "[5 rows x 46 columns]\n",
      "\n",
      "Missing values in the dataset:\n",
      "Series([], dtype: int64)\n",
      "\n",
      "Rows after dropping missing values: 668522\n",
      "\n",
      "Available columns: ['pkSeqID', 'stime', 'flgs', 'flgs_number', 'proto', 'proto_number', 'saddr', 'sport', 'daddr', 'dport', 'pkts', 'bytes', 'state', 'state_number', 'ltime', 'seq', 'dur', 'mean', 'stddev', 'sum', 'min', 'max', 'spkts', 'dpkts', 'sbytes', 'dbytes', 'rate', 'srate', 'drate', 'TnBPSrcIP', 'TnBPDstIP', 'TnP_PSrcIP', 'TnP_PDstIP', 'TnP_PerProto', 'TnP_Per_Dport', 'AR_P_Proto_P_SrcIP', 'AR_P_Proto_P_DstIP', 'N_IN_Conn_P_DstIP', 'N_IN_Conn_P_SrcIP', 'AR_P_Proto_P_Sport', 'AR_P_Proto_P_Dport', 'Pkts_P_State_P_Protocol_P_DestIP', 'Pkts_P_State_P_Protocol_P_SrcIP', 'attack', 'category', 'subcategory']\n",
      "\n",
      "Original target distribution:\n",
      " stime\n",
      "1528099353    103599\n",
      "1528099338     92175\n",
      "1528099352     83336\n",
      "1528099367     78520\n",
      "1528099368     50774\n",
      "               ...  \n",
      "1526347390         1\n",
      "1529380769         1\n",
      "1526346035         1\n",
      "1526347023         1\n",
      "1526350750         1\n",
      "Name: count, Length: 5586, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\paras\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 35.6 MiB for an array with shape (103596, 45) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 67\u001b[0m\n\u001b[0;32m     64\u001b[0m k_neighbors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;241m5\u001b[39m, min_class_size \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     66\u001b[0m smote \u001b[38;5;241m=\u001b[39m SMOTE(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, k_neighbors\u001b[38;5;241m=\u001b[39mk_neighbors)\n\u001b[1;32m---> 67\u001b[0m X_train_balanced, y_train_balanced \u001b[38;5;241m=\u001b[39m smote\u001b[38;5;241m.\u001b[39mfit_resample(X_train_filtered, y_train_filtered)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBalanced target distribution:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, y_train_balanced\u001b[38;5;241m.\u001b[39mvalue_counts())\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# 6: Visualize balanced target distribution\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\paras\\anaconda3\\Lib\\site-packages\\imblearn\\base.py:208\u001b[0m, in \u001b[0;36mBaseSampler.fit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Resample the dataset.\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \n\u001b[0;32m    189\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;124;03m    The corresponding label of `X_resampled`.\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m--> 208\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit_resample(X, y)\n",
      "File \u001b[1;32mc:\\Users\\paras\\anaconda3\\Lib\\site-packages\\imblearn\\base.py:112\u001b[0m, in \u001b[0;36mSamplerMixin.fit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    106\u001b[0m X, y, binarize_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_X_y(X, y)\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy_ \u001b[38;5;241m=\u001b[39m check_sampling_strategy(\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy, y, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampling_type\n\u001b[0;32m    110\u001b[0m )\n\u001b[1;32m--> 112\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_resample(X, y)\n\u001b[0;32m    114\u001b[0m y_ \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    115\u001b[0m     label_binarize(output[\u001b[38;5;241m1\u001b[39m], classes\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39munique(y)) \u001b[38;5;28;01mif\u001b[39;00m binarize_y \u001b[38;5;28;01melse\u001b[39;00m output[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    116\u001b[0m )\n\u001b[0;32m    118\u001b[0m X_, y_ \u001b[38;5;241m=\u001b[39m arrays_transformer\u001b[38;5;241m.\u001b[39mtransform(output[\u001b[38;5;241m0\u001b[39m], y_)\n",
      "File \u001b[1;32mc:\\Users\\paras\\anaconda3\\Lib\\site-packages\\imblearn\\over_sampling\\_smote\\base.py:390\u001b[0m, in \u001b[0;36mSMOTE._fit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_k_\u001b[38;5;241m.\u001b[39mfit(X_class)\n\u001b[0;32m    389\u001b[0m nns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_k_\u001b[38;5;241m.\u001b[39mkneighbors(X_class, return_distance\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[:, \u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m--> 390\u001b[0m X_new, y_new \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_samples(\n\u001b[0;32m    391\u001b[0m     X_class, y\u001b[38;5;241m.\u001b[39mdtype, class_sample, X_class, nns, n_samples, \u001b[38;5;241m1.0\u001b[39m\n\u001b[0;32m    392\u001b[0m )\n\u001b[0;32m    393\u001b[0m X_resampled\u001b[38;5;241m.\u001b[39mappend(X_new)\n\u001b[0;32m    394\u001b[0m y_resampled\u001b[38;5;241m.\u001b[39mappend(y_new)\n",
      "File \u001b[1;32mc:\\Users\\paras\\anaconda3\\Lib\\site-packages\\imblearn\\over_sampling\\_smote\\base.py:128\u001b[0m, in \u001b[0;36mBaseSMOTE._make_samples\u001b[1;34m(self, X, y_dtype, y_type, nn_data, nn_num, n_samples, step_size, y)\u001b[0m\n\u001b[0;32m    125\u001b[0m rows \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfloor_divide(samples_indices, nn_num\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    126\u001b[0m cols \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmod(samples_indices, nn_num\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m--> 128\u001b[0m X_new \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_samples(X, nn_data, nn_num, rows, cols, steps, y_type, y)\n\u001b[0;32m    129\u001b[0m y_new \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfull(n_samples, fill_value\u001b[38;5;241m=\u001b[39my_type, dtype\u001b[38;5;241m=\u001b[39my_dtype)\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X_new, y_new\n",
      "File \u001b[1;32mc:\\Users\\paras\\anaconda3\\Lib\\site-packages\\imblearn\\over_sampling\\_smote\\base.py:195\u001b[0m, in \u001b[0;36mBaseSMOTE._generate_samples\u001b[1;34m(self, X, nn_data, nn_num, rows, cols, steps, y_type, y)\u001b[0m\n\u001b[0;32m    193\u001b[0m     X_new \u001b[38;5;241m=\u001b[39m X[rows] \u001b[38;5;241m+\u001b[39m steps\u001b[38;5;241m.\u001b[39mmultiply(diffs)\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 195\u001b[0m     X_new \u001b[38;5;241m=\u001b[39m X[rows] \u001b[38;5;241m+\u001b[39m steps \u001b[38;5;241m*\u001b[39m diffs\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X_new\u001b[38;5;241m.\u001b[39mastype(X\u001b[38;5;241m.\u001b[39mdtype)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 35.6 MiB for an array with shape (103596, 45) and data type float64"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# 1: Load the dataset\n",
    "file_path = r'C:\\Users\\paras\\Documents\\UNSW_2018_IoT_Botnet_Full5pc_4.csv'\n",
    "data = pd.read_csv(r\"C:\\Users\\paras\\Documents\\UNSW_2018_IoT_Botnet_Full5pc_4.csv\")\n",
    "print(\"Dataset loaded successfully.\")\n",
    "print(data.head())\n",
    "\n",
    "# 2: Check for missing values\n",
    "print(\"\\nMissing values in the dataset:\")\n",
    "missing_values = data.isnull().sum()\n",
    "print(missing_values[missing_values > 0])\n",
    "\n",
    "# Drop rows with missing values\n",
    "data_cleaned = data.dropna()\n",
    "print(\"\\nRows after dropping missing values:\", data_cleaned.shape[0])\n",
    "\n",
    "# Check available column names\n",
    "print(\"\\nAvailable columns:\", data_cleaned.columns.tolist())\n",
    "\n",
    "# 3: Encode categorical variables\n",
    "label_encoders = {}\n",
    "for column in data_cleaned.select_dtypes(include=['object']).columns:\n",
    "    le = LabelEncoder()\n",
    "    data_cleaned.loc[:, column] = le.fit_transform(data_cleaned[column].astype(str))\n",
    "    label_encoders[column] = le\n",
    "\n",
    "# Target column is \"stime\"\n",
    "target_column = 'stime'\n",
    "\n",
    "# 4: Define features and target\n",
    "data_cleaned[target_column] = data_cleaned[target_column].astype(int)\n",
    "X = data_cleaned.drop(columns=[target_column])\n",
    "y = data_cleaned[target_column]\n",
    "\n",
    "# Normalize and scale the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 5: Balance the training set using SMOTE (improved handling of rare classes)\n",
    "print(\"\\nOriginal target distribution:\\n\", y_train.value_counts())\n",
    "\n",
    "# Drop classes with fewer than 2 samples (can't be used with SMOTE)\n",
    "class_counts = y_train.value_counts()\n",
    "valid_classes = class_counts[class_counts > 1].index\n",
    "\n",
    "X_train_filtered = X_train[y_train.isin(valid_classes)]\n",
    "y_train_filtered = y_train[y_train.isin(valid_classes)]\n",
    "\n",
    "# Adjust k_neighbors based on smallest class size\n",
    "min_class_size = y_train_filtered.value_counts().min()\n",
    "k_neighbors = min(5, min_class_size - 1)\n",
    "\n",
    "smote = SMOTE(random_state=42, k_neighbors=k_neighbors)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train_filtered, y_train_filtered)\n",
    "\n",
    "print(\"\\nBalanced target distribution:\\n\", y_train_balanced.value_counts())\n",
    "\n",
    "# 6: Visualize balanced target distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(y_train_balanced, bins=10, edgecolor='k')\n",
    "plt.title('Distribution of Target Variable (After SMOTE)')\n",
    "plt.xlabel('Target Variable')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n",
    "\n",
    "# 7: Correlation-based feature selection\n",
    "data_balanced = pd.DataFrame(X_train_balanced, columns=X.columns)\n",
    "data_balanced[target_column] = y_train_balanced\n",
    "correlation_matrix = data_balanced.corr()\n",
    "\n",
    "print(\"\\nCorrelation matrix:\")\n",
    "print(correlation_matrix)\n",
    "\n",
    "threshold = 0.5\n",
    "selected_features = correlation_matrix[abs(correlation_matrix[target_column]) > threshold].index.tolist()\n",
    "selected_features.remove(target_column)\n",
    "print(\"\\nSelected features based on correlation threshold:\")\n",
    "print(selected_features)\n",
    "\n",
    "# 8: Wrapper method (RFE)\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "rfe = RFE(model, n_features_to_select=5)\n",
    "rfe.fit(X_train_balanced, y_train_balanced)\n",
    "selected_rfe_features = X.columns[rfe.support_]\n",
    "print(\"\\nSelected features using RFE:\")\n",
    "print(selected_rfe_features)\n",
    "\n",
    "# 9: Embedded method - Feature importance\n",
    "model.fit(X_train_balanced, y_train_balanced)\n",
    "importances = model.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "print(\"\\nFeature ranking (Embedded method):\")\n",
    "for i in range(len(X.columns)):\n",
    "    print(f\"{i + 1}. Feature {X.columns[indices[i]]} - Importance: {importances[indices[i]]:.4f}\")\n",
    "\n",
    "# 10: Train final classifier and evaluate\n",
    "classifier = RandomForestClassifier(random_state=42)\n",
    "classifier.fit(X_train_balanced, y_train_balanced)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "accuracy = classifier.score(X_test, y_test)\n",
    "print(\"\\nAccuracy of the Random Forest Classifier:\", accuracy)\n",
    "\n",
    "# Additional evaluation metrics\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
